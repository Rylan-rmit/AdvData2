{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Rylan Pease\n",
    "#### Student ID: s3896416\n",
    "\n",
    "Date: 1/10/23\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* os\n",
    "* collections\n",
    "\n",
    "## Introduction\n",
    "In this doccument I will be pre-processing the data as well as creating the vocab.txt file\n",
    "\n",
    "<span style=\"color: red\"> Note that this is a sample notebook only. You will need to fill in the proper markdown and code blocks. You might also want to make necessary changes to the structure to meet your own needs. Note also that any generic comments written in this notebook are to be removed and replace with your own words.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "import os\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- xamine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Subdirectories: 4\n",
      "Total Data Points: 776\n"
     ]
    }
   ],
   "source": [
    "# Code to inspect the provided data file...\n",
    "\n",
    "sub_dirs = os.listdir('data')\n",
    "print('Total Subdirectories: ' + str(len(sub_dirs)))\n",
    "\n",
    "dataFiles = []\n",
    "for i in sub_dirs:\n",
    "    inserting = []\n",
    "    inserting.extend(os.listdir('data/'+i))\n",
    "    dataFiles.append(inserting)\n",
    "\n",
    "dataDirs = []\n",
    "for i in range(len(dataFiles)):\n",
    "    for j in range(len(dataFiles[i])):\n",
    "        dataDirs.append('data/'+sub_dirs[i]+'/'+dataFiles[i][j])\n",
    "    \n",
    "print('Total Data Points: ' + str(len(dataDirs)))\n",
    "\n",
    "# Code to read in the data file...\n",
    "\n",
    "textList = []\n",
    "\n",
    "for i in range(len(dataDirs)):\n",
    "    with open(dataDirs[i], 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        textList.append(text + \"\\nCategory: \" + dataDirs[i].split('/')[1])\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for block in textList:\n",
    "    lines = block.strip().split(\"\\n\")\n",
    "    data = {}\n",
    "    for line in lines:\n",
    "        key, value = line.split(\": \", 1)\n",
    "        data[key] = value\n",
    "    data_list.append(data)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 776 entries, 0 to 775\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Title        776 non-null    object\n",
      " 1   Webindex     776 non-null    int32 \n",
      " 2   Company      776 non-null    object\n",
      " 3   Description  776 non-null    object\n",
      " 4   Category     776 non-null    object\n",
      "dtypes: int32(1), object(4)\n",
      "memory usage: 27.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set Title to string\n",
    "df['Title'] = df['Title'].astype(str)\n",
    "\n",
    "# set Webindex to int\n",
    "df['Webindex'] = df['Webindex'].astype(int)\n",
    "\n",
    "# set Company as string\n",
    "df['Company'] = df['Company'].astype(str)\n",
    "\n",
    "# set Description as string\n",
    "df['Description'] = df['Description'].astype(str)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on basic text pre-processing\n",
    "\n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression for word tokenization\n",
    "token_pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "\n",
    "# Tokenize the description column after adding to the DataFrame\n",
    "df['Tokenized Description'] = df['Description'].apply(lambda x: [word.lower() for word in re.findall(token_pattern, x) if len(word) > 1])\n",
    "\n",
    "with open(\"stopwords_en.txt\", \"r\") as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Remove stopwords\n",
    "df['Tokenized Description'] = df['Tokenized Description'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "\n",
    "# Count term frequencies across all documents\n",
    "term_frequencies = Counter(word for words in df['Tokenized Description'] for word in words)\n",
    "\n",
    "# Remove words that appear only once\n",
    "df['Tokenized Description'] = df['Tokenized Description'].apply(lambda x: [word for word in x if term_frequencies[word] > 1])\n",
    "\n",
    "# Compute document frequencies\n",
    "document_frequencies = defaultdict(int)\n",
    "for words in df['Tokenized Description']:\n",
    "    for word in set(words):  # Convert to set to ensure each word is counted once per document\n",
    "        document_frequencies[word] += 1\n",
    "\n",
    "# Get the top 50 most frequent words based on document frequency\n",
    "top_50_words = {word for word, freq in Counter(document_frequencies).most_common(50)}\n",
    "\n",
    "# Remove the top 50 words from each document\n",
    "df['Tokenized Description'] = df['Tokenized Description'].apply(lambda x: [word for word in x if word not in top_50_words])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"processed_job_ads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all unique words from the tokenized descriptions\n",
    "vocabulary = sorted(set(word for words in df['Tokenized Description'] for word in words))\n",
    "\n",
    "# Assign an integer index to each word\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Save the vocabulary to a text file\n",
    "with open(\"vocab.txt\", \"w\") as f:\n",
    "    for word, idx in vocab_dict.items():\n",
    "        f.write(f\"{word}:{idx}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
